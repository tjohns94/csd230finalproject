{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhis_data_all = pd.read_csv('./data/nhis_data_all.csv', low_memory=False)\n",
    "\n",
    "# Replace oral_exam and dental_caries values with 0 or 1\n",
    "nhis_corrected_dict = {\n",
    "    'oral_exam': {\n",
    "        0: 0,\n",
    "        1: 1,\n",
    "        '0': 0,\n",
    "        '1': 1,\n",
    "        'N': 0,\n",
    "        'Y': 1\n",
    "    },\n",
    "    \n",
    "    'dental_caries': {\n",
    "        0: 0,\n",
    "        1: 1,\n",
    "        'N': 0,\n",
    "        'Y': 1\n",
    "    }\n",
    "}\n",
    "nhis_data_all['oral_exam'] = nhis_data_all['oral_exam'].map(nhis_corrected_dict['oral_exam'])\n",
    "nhis_data_all['dental_caries'] = nhis_data_all['dental_caries'].map(nhis_corrected_dict['dental_caries'])\n",
    "\n",
    "# Create waist_over_weight column\n",
    "nhis_data_all['waist_over_weight'] = nhis_data_all['waist_circum'] / nhis_data_all['weight']\n",
    "\n",
    "# If waist_over_weight is less than 0.5 or greater than 3.0, set waist_circum to nan\n",
    "nhis_data_all.loc[(nhis_data_all['waist_over_weight'] < 0.5) | (nhis_data_all['waist_over_weight'] > 3.0) | (nhis_data_all['waist_circum'] == 999), 'waist_circum'] = np.nan\n",
    "\n",
    "# If waist_circum or weight is nan, set waist_over_weight to nan\n",
    "nhis_data_all.loc[(nhis_data_all['waist_circum'].isnull()) | (nhis_data_all['weight'].isnull()), 'waist_over_weight'] = np.nan\n",
    "\n",
    "# Change left_eye and right_eye value 9.9 to 3.0\n",
    "nhis_data_all.loc[nhis_data_all['left_eye'] == 9.9, 'left_eye'] = 3.0\n",
    "nhis_data_all.loc[nhis_data_all['right_eye'] == 9.9, 'right_eye'] = 3.0\n",
    "\n",
    "# Change left_ear and right_ear value 3.0 to nan\n",
    "nhis_data_all.loc[nhis_data_all['left_ear'] == 3.0, 'left_ear'] = np.nan\n",
    "nhis_data_all.loc[nhis_data_all['right_ear'] == 3.0, 'right_ear'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for feature: age_code\n",
      "Logistic Regression Results:\n",
      "[[1212145   43570       0]\n",
      " [ 559852   33044       0]\n",
      " [ 138934   12455       0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.97      0.77   1255715\n",
      "           1       0.37      0.06      0.10    592896\n",
      "           2       0.00      0.00      0.00    151389\n",
      "\n",
      "    accuracy                           0.62   2000000\n",
      "   macro avg       0.34      0.34      0.29   2000000\n",
      "weighted avg       0.51      0.62      0.51   2000000\n",
      "\n",
      "Decision Tree Results:\n",
      "[[1255715       0       0]\n",
      " [ 592896       0       0]\n",
      " [ 151389       0       0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77   1255715\n",
      "           1       0.00      0.00      0.00    592896\n",
      "           2       0.00      0.00      0.00    151389\n",
      "\n",
      "    accuracy                           0.63   2000000\n",
      "   macro avg       0.21      0.33      0.26   2000000\n",
      "weighted avg       0.39      0.63      0.48   2000000\n",
      "\n",
      "Random Forest Results:\n",
      "[[1255715       0       0]\n",
      " [ 592896       0       0]\n",
      " [ 151389       0       0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77   1255715\n",
      "           1       0.00      0.00      0.00    592896\n",
      "           2       0.00      0.00      0.00    151389\n",
      "\n",
      "    accuracy                           0.63   2000000\n",
      "   macro avg       0.21      0.33      0.26   2000000\n",
      "weighted avg       0.39      0.63      0.48   2000000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Results for feature: total_cholesterol\n",
      "Logistic Regression Results:\n",
      "[[842281     61      0]\n",
      " [393758     37      0]\n",
      " [102908     40      0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77    842342\n",
      "           1       0.27      0.00      0.00    393795\n",
      "           2       0.00      0.00      0.00    102948\n",
      "\n",
      "    accuracy                           0.63   1339085\n",
      "   macro avg       0.30      0.33      0.26   1339085\n",
      "weighted avg       0.47      0.63      0.49   1339085\n",
      "\n",
      "Decision Tree Results:\n",
      "[[842129    193     20]\n",
      " [393631    147     17]\n",
      " [102846     83     19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77    842342\n",
      "           1       0.35      0.00      0.00    393795\n",
      "           2       0.34      0.00      0.00    102948\n",
      "\n",
      "    accuracy                           0.63   1339085\n",
      "   macro avg       0.44      0.33      0.26   1339085\n",
      "weighted avg       0.52      0.63      0.49   1339085\n",
      "\n",
      "Random Forest Results:\n",
      "[[842058    259     25]\n",
      " [393579    196     20]\n",
      " [102814    113     21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77    842342\n",
      "           1       0.35      0.00      0.00    393795\n",
      "           2       0.32      0.00      0.00    102948\n",
      "\n",
      "    accuracy                           0.63   1339085\n",
      "   macro avg       0.43      0.33      0.26   1339085\n",
      "weighted avg       0.52      0.63      0.49   1339085\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Results for feature: triglycerides\n",
      "Logistic Regression Results:\n",
      "[[821811  18893    524]\n",
      " [376851  17250    504]\n",
      " [ 94236   8614    399]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.98      0.77    841228\n",
      "           1       0.39      0.04      0.08    394605\n",
      "           2       0.28      0.00      0.01    103249\n",
      "\n",
      "    accuracy                           0.63   1339082\n",
      "   macro avg       0.43      0.34      0.29   1339082\n",
      "weighted avg       0.53      0.63      0.51   1339082\n",
      "\n",
      "Decision Tree Results:\n",
      "[[837731   3367    130]\n",
      " [391134   3342    129]\n",
      " [101262   1898     89]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77    841228\n",
      "           1       0.39      0.01      0.02    394605\n",
      "           2       0.26      0.00      0.00    103249\n",
      "\n",
      "    accuracy                           0.63   1339082\n",
      "   macro avg       0.42      0.34      0.26   1339082\n",
      "weighted avg       0.53      0.63      0.49   1339082\n",
      "\n",
      "Random Forest Results:\n",
      "[[837507   3564    157]\n",
      " [390923   3521    161]\n",
      " [101123   2014    112]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77    841228\n",
      "           1       0.39      0.01      0.02    394605\n",
      "           2       0.26      0.00      0.00    103249\n",
      "\n",
      "    accuracy                           0.63   1339082\n",
      "   macro avg       0.43      0.34      0.26   1339082\n",
      "weighted avg       0.53      0.63      0.49   1339082\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Results for feature: hdl_cholesterol\n",
      "Logistic Regression Results:\n",
      "[[842579      0      0]\n",
      " [393660      0      0]\n",
      " [102840      0      0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77    842579\n",
      "           1       0.00      0.00      0.00    393660\n",
      "           2       0.00      0.00      0.00    102840\n",
      "\n",
      "    accuracy                           0.63   1339079\n",
      "   macro avg       0.21      0.33      0.26   1339079\n",
      "weighted avg       0.40      0.63      0.49   1339079\n",
      "\n",
      "Decision Tree Results:\n",
      "[[842475     83     21]\n",
      " [393593     45     22]\n",
      " [102811     19     10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77    842579\n",
      "           1       0.31      0.00      0.00    393660\n",
      "           2       0.19      0.00      0.00    102840\n",
      "\n",
      "    accuracy                           0.63   1339079\n",
      "   macro avg       0.37      0.33      0.26   1339079\n",
      "weighted avg       0.50      0.63      0.49   1339079\n",
      "\n",
      "Random Forest Results:\n",
      "[[842458     99     22]\n",
      " [393585     51     24]\n",
      " [102807     23     10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77    842579\n",
      "           1       0.29      0.00      0.00    393660\n",
      "           2       0.18      0.00      0.00    102840\n",
      "\n",
      "    accuracy                           0.63   1339079\n",
      "   macro avg       0.37      0.33      0.26   1339079\n",
      "weighted avg       0.50      0.63      0.49   1339079\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Results for feature: ldl_cholesterol\n",
      "Logistic Regression Results:\n",
      "[[836909    119      0]\n",
      " [389964     86      0]\n",
      " [101603     19      0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77    837028\n",
      "           1       0.38      0.00      0.00    390050\n",
      "           2       0.00      0.00      0.00    101622\n",
      "\n",
      "    accuracy                           0.63   1328700\n",
      "   macro avg       0.34      0.33      0.26   1328700\n",
      "weighted avg       0.51      0.63      0.49   1328700\n",
      "\n",
      "Decision Tree Results:\n",
      "[[836849    146     33]\n",
      " [389898    119     33]\n",
      " [101522     73     27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77    837028\n",
      "           1       0.35      0.00      0.00    390050\n",
      "           2       0.29      0.00      0.00    101622\n",
      "\n",
      "    accuracy                           0.63   1328700\n",
      "   macro avg       0.42      0.33      0.26   1328700\n",
      "weighted avg       0.52      0.63      0.49   1328700\n",
      "\n",
      "Random Forest Results:\n",
      "[[836819    158     51]\n",
      " [389878    126     46]\n",
      " [101509     75     38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77    837028\n",
      "           1       0.35      0.00      0.00    390050\n",
      "           2       0.28      0.00      0.00    101622\n",
      "\n",
      "    accuracy                           0.63   1328700\n",
      "   macro avg       0.42      0.33      0.26   1328700\n",
      "weighted avg       0.52      0.63      0.49   1328700\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Results for feature: systolic_bp\n",
      "Logistic Regression Results:\n",
      "[[1209725   40382      11]\n",
      " [ 554644   38641      13]\n",
      " [ 135319   15704      11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.97      0.77   1250118\n",
      "           1       0.41      0.07      0.11    593298\n",
      "           2       0.31      0.00      0.00    151034\n",
      "\n",
      "    accuracy                           0.63   1994450\n",
      "   macro avg       0.45      0.34      0.29   1994450\n",
      "weighted avg       0.54      0.63      0.51   1994450\n",
      "\n",
      "Decision Tree Results:\n",
      "[[1241265    8852       1]\n",
      " [ 584354    8943       1]\n",
      " [ 147040    3992       2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.99      0.77   1250118\n",
      "           1       0.41      0.02      0.03    593298\n",
      "           2       0.50      0.00      0.00    151034\n",
      "\n",
      "    accuracy                           0.63   1994450\n",
      "   macro avg       0.51      0.34      0.27   1994450\n",
      "weighted avg       0.55      0.63      0.49   1994450\n",
      "\n",
      "Random Forest Results:\n",
      "[[1241265    8851       2]\n",
      " [ 584354    8943       1]\n",
      " [ 147039    3992       3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.99      0.77   1250118\n",
      "           1       0.41      0.02      0.03    593298\n",
      "           2       0.50      0.00      0.00    151034\n",
      "\n",
      "    accuracy                           0.63   1994450\n",
      "   macro avg       0.51      0.34      0.27   1994450\n",
      "weighted avg       0.55      0.63      0.49   1994450\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Results for feature: diastolic_bp\n",
      "Logistic Regression Results:\n",
      "[[1229018   20643       0]\n",
      " [ 575415   18335       0]\n",
      " [ 145117    5922       0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.98      0.77   1249661\n",
      "           1       0.41      0.03      0.06    593750\n",
      "           2       0.00      0.00      0.00    151039\n",
      "\n",
      "    accuracy                           0.63   1994450\n",
      "   macro avg       0.35      0.34      0.28   1994450\n",
      "weighted avg       0.52      0.63      0.50   1994450\n",
      "\n",
      "Decision Tree Results:\n",
      "[[1248764     897       0]\n",
      " [ 592777     972       1]\n",
      " [ 150697     342       0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77   1249661\n",
      "           1       0.44      0.00      0.00    593750\n",
      "           2       0.00      0.00      0.00    151039\n",
      "\n",
      "    accuracy                           0.63   1994450\n",
      "   macro avg       0.36      0.33      0.26   1994450\n",
      "weighted avg       0.52      0.63      0.48   1994450\n",
      "\n",
      "Random Forest Results:\n",
      "[[1248695     965       1]\n",
      " [ 592711    1037       2]\n",
      " [ 150667     370       2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77   1249661\n",
      "           1       0.44      0.00      0.00    593750\n",
      "           2       0.40      0.00      0.00    151039\n",
      "\n",
      "    accuracy                           0.63   1994450\n",
      "   macro avg       0.49      0.33      0.26   1994450\n",
      "weighted avg       0.55      0.63      0.48   1994450\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Results for feature: serum_creatinine\n",
      "Logistic Regression Results:\n",
      "[[1247081    1820      30]\n",
      " [ 592952    1079      14]\n",
      " [ 150596     786       3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77   1248931\n",
      "           1       0.29      0.00      0.00    594045\n",
      "           2       0.06      0.00      0.00    151385\n",
      "\n",
      "    accuracy                           0.63   1994361\n",
      "   macro avg       0.33      0.33      0.26   1994361\n",
      "weighted avg       0.48      0.63      0.48   1994361\n",
      "\n",
      "Decision Tree Results:\n",
      "[[1248917      11       3]\n",
      " [ 594039       6       0]\n",
      " [ 151381       3       1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77   1248931\n",
      "           1       0.30      0.00      0.00    594045\n",
      "           2       0.25      0.00      0.00    151385\n",
      "\n",
      "    accuracy                           0.63   1994361\n",
      "   macro avg       0.39      0.33      0.26   1994361\n",
      "weighted avg       0.50      0.63      0.48   1994361\n",
      "\n",
      "Random Forest Results:\n",
      "[[1248901      23       7]\n",
      " [ 594034       8       3]\n",
      " [ 151381       3       1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77   1248931\n",
      "           1       0.24      0.00      0.00    594045\n",
      "           2       0.09      0.00      0.00    151385\n",
      "\n",
      "    accuracy                           0.63   1994361\n",
      "   macro avg       0.32      0.33      0.26   1994361\n",
      "weighted avg       0.47      0.63      0.48   1994361\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Results for feature: weight\n",
      "Logistic Regression Results:\n",
      "[[1242132   12219       0]\n",
      " [ 583960   10120       0]\n",
      " [ 147727    3663       0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.99      0.77   1254351\n",
      "           1       0.39      0.02      0.03    594080\n",
      "           2       0.00      0.00      0.00    151390\n",
      "\n",
      "    accuracy                           0.63   1999821\n",
      "   macro avg       0.34      0.34      0.27   1999821\n",
      "weighted avg       0.51      0.63      0.49   1999821\n",
      "\n",
      "Decision Tree Results:\n",
      "[[1254202     147       2]\n",
      " [ 593926     153       1]\n",
      " [ 151285     104       1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77   1254351\n",
      "           1       0.38      0.00      0.00    594080\n",
      "           2       0.25      0.00      0.00    151390\n",
      "\n",
      "    accuracy                           0.63   1999821\n",
      "   macro avg       0.42      0.33      0.26   1999821\n",
      "weighted avg       0.52      0.63      0.48   1999821\n",
      "\n",
      "Random Forest Results:\n",
      "[[1254202     147       2]\n",
      " [ 593925     153       2]\n",
      " [ 151285     104       1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77   1254351\n",
      "           1       0.38      0.00      0.00    594080\n",
      "           2       0.20      0.00      0.00    151390\n",
      "\n",
      "    accuracy                           0.63   1999821\n",
      "   macro avg       0.40      0.33      0.26   1999821\n",
      "weighted avg       0.52      0.63      0.48   1999821\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Results for feature: waist_circum\n",
      "Logistic Regression Results:\n",
      "[[1205073   50092      70]\n",
      " [ 545580   47226      78]\n",
      " [ 131411   19831      51]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.96      0.77   1255235\n",
      "           1       0.40      0.08      0.13    592884\n",
      "           2       0.26      0.00      0.00    151293\n",
      "\n",
      "    accuracy                           0.63   1999412\n",
      "   macro avg       0.43      0.35      0.30   1999412\n",
      "weighted avg       0.54      0.63      0.52   1999412\n",
      "\n",
      "Decision Tree Results:\n",
      "[[1250797    4424      14]\n",
      " [ 588502    4365      17]\n",
      " [ 149113    2173       7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77   1255235\n",
      "           1       0.40      0.01      0.01    592884\n",
      "           2       0.18      0.00      0.00    151293\n",
      "\n",
      "    accuracy                           0.63   1999412\n",
      "   macro avg       0.40      0.33      0.26   1999412\n",
      "weighted avg       0.53      0.63      0.49   1999412\n",
      "\n",
      "Random Forest Results:\n",
      "[[1250723    4494      18]\n",
      " [ 588420    4443      21]\n",
      " [ 149094    2188      11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77   1255235\n",
      "           1       0.40      0.01      0.01    592884\n",
      "           2       0.22      0.00      0.00    151293\n",
      "\n",
      "    accuracy                           0.63   1999412\n",
      "   macro avg       0.42      0.33      0.26   1999412\n",
      "weighted avg       0.53      0.63      0.49   1999412\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Assuming the dataset is already loaded into a DataFrame named 'nhis_data_all'\n",
    "# Define the target variable\n",
    "# Create a new column 'diabetes_status' based on fasting glucose levels\n",
    "def diabetes_status(fasting_glucose):\n",
    "    if fasting_glucose >= 126:\n",
    "        return 2  # Diabetes\n",
    "    elif 100 <= fasting_glucose <= 125:\n",
    "        return 1  # Pre-diabetes\n",
    "    else:\n",
    "        return 0  # Normal\n",
    "\n",
    "nhis_data_all['diabetes_status'] = nhis_data_all['fasting_glucose'].apply(diabetes_status)\n",
    "\n",
    "# Feature Selection\n",
    "# Selecting relevant features\n",
    "features = [\n",
    "    'age_code', 'total_cholesterol', 'triglycerides', 'hdl_cholesterol', \n",
    "    'ldl_cholesterol', 'systolic_bp', 'diastolic_bp', 'serum_creatinine', \n",
    "    'weight', 'waist_circum'\n",
    "]\n",
    "\n",
    "# Iterate over each feature and train models\n",
    "results = {}\n",
    "for feature in features:\n",
    "    # Drop rows where the feature is NaN\n",
    "    nhis_data_all_feature = pd.DataFrame()\n",
    "    nhis_data_all_feature[feature] = nhis_data_all[feature]\n",
    "    nhis_data_all_feature['diabetes_status'] = nhis_data_all['diabetes_status']\n",
    "    nhis_data_all_feature = nhis_data_all_feature.dropna()\n",
    "    \n",
    "    # Split the data into features (X) and target (y)\n",
    "    X = nhis_data_all_feature[[feature]]\n",
    "    y = nhis_data_all_feature['diabetes_status']\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Store results for each feature\n",
    "    results[feature] = {}\n",
    "    \n",
    "    # Logistic Regression\n",
    "    log_reg = LogisticRegression(max_iter=1000)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    y_pred_log_reg = log_reg.predict(X_test)\n",
    "    results[feature]['Logistic Regression'] = classification_report(y_test, y_pred_log_reg, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Decision Tree\n",
    "    dec_tree = DecisionTreeClassifier()\n",
    "    dec_tree.fit(X_train, y_train)\n",
    "    y_pred_dec_tree = dec_tree.predict(X_test)\n",
    "    results[feature]['Decision Tree'] = classification_report(y_test, y_pred_dec_tree, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Random Forest\n",
    "    rand_forest = RandomForestClassifier(n_estimators=100)\n",
    "    rand_forest.fit(X_train, y_train)\n",
    "    y_pred_rand_forest = rand_forest.predict(X_test)\n",
    "    results[feature]['Random Forest'] = classification_report(y_test, y_pred_rand_forest, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Print the results for each feature\n",
    "    print(f\"Results for feature: {feature}\")\n",
    "    print(\"Logistic Regression Results:\")\n",
    "    print(confusion_matrix(y_test, y_pred_log_reg))\n",
    "    print(classification_report(y_test, y_pred_log_reg, zero_division=0))\n",
    "    print(\"Decision Tree Results:\")\n",
    "    print(confusion_matrix(y_test, y_pred_dec_tree))\n",
    "    print(classification_report(y_test, y_pred_dec_tree, zero_division=0))\n",
    "    print(\"Random Forest Results:\")\n",
    "    print(confusion_matrix(y_test, y_pred_rand_forest))\n",
    "    print(classification_report(y_test, y_pred_rand_forest, zero_division=0))\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Optionally, visualize the results or save them for further analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a representative sample of the data with 5% of the data\n",
    "nhis_data_sample = nhis_data_all.sample(frac=0.05, random_state=42)\n",
    "\n",
    "# Save the sample data to a CSV file\n",
    "nhis_data_sample.to_csv('./data/nhis_data_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
